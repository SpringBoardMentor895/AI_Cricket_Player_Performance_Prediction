{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPL Cricket Player Performance Prediction - Feature Engineering\n",
    "\n",
    "This notebook performs comprehensive feature engineering for IPL cricket player performance prediction.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. Load and prepare data\n",
    "2. Aggregate to player-match level\n",
    "3. Engineer advanced features (PUT, rolling, venue, PvP, career)\n",
    "4. Create target labels\n",
    "5. Feature selection and preprocessing\n",
    "6. Time-series aware train-test split\n",
    "7. Create feature pipeline\n",
    "8. Save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:49.633198Z",
     "iopub.status.busy": "2026-02-18T09:39:49.631955Z",
     "iopub.status.idle": "2026-02-18T09:39:53.104462Z",
     "shell.execute_reply": "2026-02-18T09:39:53.102856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:53.110982Z",
     "iopub.status.busy": "2026-02-18T09:39:53.110323Z",
     "iopub.status.idle": "2026-02-18T09:39:55.297655Z",
     "shell.execute_reply": "2026-02-18T09:39:55.295950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. LOADING AND PREPARING DATA\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Shape: (179078, 46)\n",
      "Date range: 2008-04-18 00:00:00 to 2017-05-21 00:00:00\n",
      "Unique batters: 516\n",
      "Unique venues: 41\n",
      "Unique bowling teams: 13\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load merged data and prepare for feature engineering\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. LOADING AND PREPARING DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load merged data\n",
    "    df = pd.read_csv(\"data/merged_data.csv\")\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    # Rename batsman to batter for consistency\n",
    "    df[\"batter\"] = df[\"batsman\"]\n",
    "    \n",
    "    print(f\"Data loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Date range: {df[\"date\"].min()} to {df[\"date\"].max()}\")\n",
    "    print(f\"Unique batters: {df[\"batter\"].nunique()}\")\n",
    "    print(f\"Unique venues: {df[\"venue\"].nunique()}\")\n",
    "    print(f\"Unique bowling teams: {df[\"bowling_team\"].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aggregating to Player-Match Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:55.304529Z",
     "iopub.status.busy": "2026-02-18T09:39:55.303889Z",
     "iopub.status.idle": "2026-02-18T09:39:55.504765Z",
     "shell.execute_reply": "2026-02-18T09:39:55.503157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. AGGREGATING TO PLAYER-MATCH LEVEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player-match level data shape: (9515, 12)\n",
      "Sample data:\n",
      "           batter  match_id       date  \\\n",
      "0  A Ashish Reddy       346 2012-04-29   \n",
      "1  A Ashish Reddy       352 2012-05-04   \n",
      "2  A Ashish Reddy       359 2012-05-08   \n",
      "3  A Ashish Reddy       373 2012-05-18   \n",
      "4  A Ashish Reddy       376 2012-05-20   \n",
      "\n",
      "                                       venue                 bowling_team  \\\n",
      "0                           Wankhede Stadium               Mumbai Indians   \n",
      "1            MA Chidambaram Stadium, Chepauk          Chennai Super Kings   \n",
      "2  Rajiv Gandhi International Stadium, Uppal                 Punjab Kings   \n",
      "3  Rajiv Gandhi International Stadium, Uppal             Rajasthan Royals   \n",
      "4  Rajiv Gandhi International Stadium, Uppal  Royal Challengers Bangalore   \n",
      "\n",
      "   batsman_runs  ball  is_wicket  is_boundary  is_six  is_four  strike_rate  \n",
      "0            10    10          1            1       1        0        100.0  \n",
      "1             3     3          1            0       0        0        100.0  \n",
      "2             8     8          1            1       0        1        100.0  \n",
      "3            10     4          0            2       0        2        250.0  \n",
      "4             4     5          1            0       0        0         80.0  \n"
     ]
    }
   ],
   "source": [
    "def aggregate_to_player_match(df):\n",
    "    \"\"\"Aggregate ball-by-ball data to player-match level\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. AGGREGATING TO PLAYER-MATCH LEVEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Aggregate to player-match level\n",
    "    player_match = df.groupby([\"batter\", \"match_id\", \"date\", \"venue\", \"bowling_team\"]).agg({\n",
    "        \"batsman_runs\": \"sum\",\n",
    "        \"ball\": \"count\",\n",
    "        \"is_wicket\": \"sum\",\n",
    "        \"is_boundary\": \"sum\",\n",
    "        \"is_six\": \"sum\",\n",
    "        \"is_four\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate strike rate\n",
    "    player_match[\"strike_rate\"] = (player_match[\"batsman_runs\"] / player_match[\"ball\"] * 100).round(2)\n",
    "    \n",
    "    # Sort by batter and date for time-series operations\n",
    "    player_match = player_match.sort_values([\"batter\", \"date\"])\n",
    "    \n",
    "    print(f\"Player-match level data shape: {player_match.shape}\")\n",
    "    print(f\"Sample data:\")\n",
    "    print(player_match.head())\n",
    "    \n",
    "    return player_match\n",
    "\n",
    "# Aggregate data to player-match level\n",
    "player_match = aggregate_to_player_match(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Engineering PUT Features (Player vs Team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:55.510612Z",
     "iopub.status.busy": "2026-02-18T09:39:55.510041Z",
     "iopub.status.idle": "2026-02-18T09:39:55.662242Z",
     "shell.execute_reply": "2026-02-18T09:39:55.660391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3.1 ENGINEERING PUT FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUT features created:\n",
      "           batter                 bowling_team  batsman_runs    put_avg  \\\n",
      "0  A Ashish Reddy               Mumbai Indians            10  13.500000   \n",
      "1  A Ashish Reddy          Chennai Super Kings             3  15.000000   \n",
      "2  A Ashish Reddy                 Punjab Kings             8  12.333333   \n",
      "3  A Ashish Reddy             Rajasthan Royals            10  12.333333   \n",
      "4  A Ashish Reddy  Royal Challengers Bangalore             4  11.000000   \n",
      "\n",
      "   put_avg_expanding  \n",
      "0           8.500000  \n",
      "1                NaN  \n",
      "2          13.000000  \n",
      "3          12.333333  \n",
      "4          12.333333  \n"
     ]
    }
   ],
   "source": [
    "def engineer_put_features(player_match):\n",
    "    \"\"\"Create Player vs Team (PUT) features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3.1 ENGINEERING PUT FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # PUT (Player vs Team) average - overall average against bowling team\n",
    "    player_match[\"put_avg\"] = player_match.groupby([\"batter\", \"bowling_team\"])[\"batsman_runs\"].transform(\"mean\")\n",
    "    \n",
    "    # PUT average with expanding mean (cumulative performance)\n",
    "    put_expanding = player_match.groupby([\"batter\", \"bowling_team\"])[\"batsman_runs\"]\\\n",
    "        .expanding().mean().shift(1, fill_value=np.nan)\n",
    "    # Reset index to match original dataframe\n",
    "    put_expanding = put_expanding.reset_index(level=[0,1], drop=True)\n",
    "    player_match[\"put_avg_expanding\"] = put_expanding\n",
    "    \n",
    "    print(\"PUT features created:\")\n",
    "    print(player_match[[\"batter\", \"bowling_team\", \"batsman_runs\", \"put_avg\", \"put_avg_expanding\"]].head())\n",
    "    \n",
    "    return player_match\n",
    "\n",
    "# Create PUT features\n",
    "player_match = engineer_put_features(player_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Engineering Rolling Form Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:55.669231Z",
     "iopub.status.busy": "2026-02-18T09:39:55.668138Z",
     "iopub.status.idle": "2026-02-18T09:39:55.793948Z",
     "shell.execute_reply": "2026-02-18T09:39:55.791241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3.2 ENGINEERING ROLLING FORM FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling form features created:\n",
      "            batter       date  batsman_runs  rolling_avg_5  rolling_sr_5\n",
      "0   A Ashish Reddy 2012-04-29            10          10.00       100.000\n",
      "1   A Ashish Reddy 2012-05-04             3           6.50       100.000\n",
      "2   A Ashish Reddy 2012-05-08             8           7.00       100.000\n",
      "3   A Ashish Reddy 2012-05-18            10           7.75       137.500\n",
      "4   A Ashish Reddy 2012-05-20             4           7.00       126.000\n",
      "5   A Ashish Reddy 2013-04-05             7           6.40       141.000\n",
      "6   A Ashish Reddy 2013-04-07            14           8.60       144.334\n",
      "14  A Ashish Reddy 2013-04-09             3           7.60       139.334\n",
      "7   A Ashish Reddy 2013-04-12            16           8.80       124.890\n",
      "8   A Ashish Reddy 2013-04-14             4           8.80       124.890\n"
     ]
    }
   ],
   "source": [
    "def engineer_rolling_features(player_match):\n",
    "    \"\"\"Create rolling form features (last 5 matches)\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3.2 ENGINEERING ROLLING FORM FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Rolling average of last 5 matches\n",
    "    player_match[\"rolling_avg_5\"] = player_match.groupby(\"batter\")[\"batsman_runs\"]\\\n",
    "        .rolling(5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Rolling strike rate of last 5 matches\n",
    "    player_match[\"rolling_sr_5\"] = player_match.groupby(\"batter\")[\"strike_rate\"]\\\n",
    "        .rolling(5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    print(\"Rolling form features created:\")\n",
    "    print(player_match[[\"batter\", \"date\", \"batsman_runs\", \"rolling_avg_5\", \"rolling_sr_5\"]].head(10))\n",
    "    \n",
    "    return player_match\n",
    "\n",
    "# Create rolling form features\n",
    "player_match = engineer_rolling_features(player_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Engineering Venue Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:55.800453Z",
     "iopub.status.busy": "2026-02-18T09:39:55.799536Z",
     "iopub.status.idle": "2026-02-18T09:39:55.831552Z",
     "shell.execute_reply": "2026-02-18T09:39:55.829977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3.3 ENGINEERING VENUE FEATURES\n",
      "============================================================\n",
      "Venue features created:\n",
      "           batter                                      venue  batsman_runs  \\\n",
      "0  A Ashish Reddy                           Wankhede Stadium            10   \n",
      "1  A Ashish Reddy            MA Chidambaram Stadium, Chepauk             3   \n",
      "2  A Ashish Reddy  Rajiv Gandhi International Stadium, Uppal             8   \n",
      "3  A Ashish Reddy  Rajiv Gandhi International Stadium, Uppal            10   \n",
      "4  A Ashish Reddy  Rajiv Gandhi International Stadium, Uppal             4   \n",
      "\n",
      "   venue_avg  venue_overall_avg  \n",
      "0  10.000000          19.321591  \n",
      "1  19.500000          20.098611  \n",
      "2   8.454545          19.611342  \n",
      "3   8.454545          19.611342  \n",
      "4   8.454545          19.611342  \n"
     ]
    }
   ],
   "source": [
    "def engineer_venue_features(player_match):\n",
    "    \"\"\"Create venue average features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3.3 ENGINEERING VENUE FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Venue average for each batter\n",
    "    player_match[\"venue_avg\"] = player_match.groupby([\"batter\", \"venue\"])[\"batsman_runs\"].transform(\"mean\")\n",
    "    \n",
    "    # Overall venue average\n",
    "    player_match[\"venue_overall_avg\"] = player_match.groupby(\"venue\")[\"batsman_runs\"].transform(\"mean\")\n",
    "    \n",
    "    print(\"Venue features created:\")\n",
    "    print(player_match[[\"batter\", \"venue\", \"batsman_runs\", \"venue_avg\", \"venue_overall_avg\"]].head())\n",
    "    \n",
    "    return player_match\n",
    "\n",
    "# Create venue features\n",
    "player_match = engineer_venue_features(player_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Engineering PvP Features (Player vs Player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:55.838035Z",
     "iopub.status.busy": "2026-02-18T09:39:55.837480Z",
     "iopub.status.idle": "2026-02-18T09:39:56.694548Z",
     "shell.execute_reply": "2026-02-18T09:39:56.692998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3.4 ENGINEERING PVP FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PvP features created:\n",
      "           batter  match_id  batsman_runs    pvp_avg\n",
      "0  A Ashish Reddy       346            10   3.888889\n",
      "1  A Ashish Reddy       352             3   3.250000\n",
      "2  A Ashish Reddy       359             8   2.000000\n",
      "3  A Ashish Reddy       373            10   4.000000\n",
      "4  A Ashish Reddy       376             4  18.000000\n"
     ]
    }
   ],
   "source": [
    "def engineer_pvp_features(player_match, df):\n",
    "    \"\"\"Create Player vs Player (PvP) features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3.4 ENGINEERING PVP FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # For PvP, we need to consider the specific bowlers faced\n",
    "    # First, get ball-by-ball data with bowler information\n",
    "    pvp_data = df.groupby([\"batter\", \"bowler\", \"match_id\"])[\"batsman_runs\"].sum().reset_index()\n",
    "    pvp_data = pvp_data.sort_values([\"batter\", \"match_id\"])\n",
    "    \n",
    "    # Calculate PvP expanding average\n",
    "    pvp_expanding = pvp_data.groupby([\"batter\", \"bowler\"])[\"batsman_runs\"]\\\n",
    "        .expanding().mean().shift(1, fill_value=np.nan)\n",
    "    pvp_expanding = pvp_expanding.reset_index(level=[0,1], drop=True)\n",
    "    pvp_data[\"pvp_avg\"] = pvp_expanding\n",
    "    \n",
    "    # Merge back to player-match level (take average of all bowlers faced in the match)\n",
    "    pvp_match_avg = pvp_data.groupby([\"batter\", \"match_id\"])[\"pvp_avg\"].mean().reset_index()\n",
    "    player_match = player_match.merge(pvp_match_avg, on=[\"batter\", \"match_id\"], how=\"left\")\n",
    "    \n",
    "    print(\"PvP features created:\")\n",
    "    print(player_match[[\"batter\", \"match_id\", \"batsman_runs\", \"pvp_avg\"]].head())\n",
    "    \n",
    "    return player_match\n",
    "\n",
    "# Create PvP features\n",
    "player_match = engineer_pvp_features(player_match, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Engineering Career Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:56.701904Z",
     "iopub.status.busy": "2026-02-18T09:39:56.701347Z",
     "iopub.status.idle": "2026-02-18T09:39:56.777866Z",
     "shell.execute_reply": "2026-02-18T09:39:56.775813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3.5 ENGINEERING CAREER FEATURES\n",
      "============================================================\n",
      "Career features created:\n",
      "           batter       date  batsman_runs  career_avg   career_sr  \\\n",
      "0  A Ashish Reddy 2012-04-29            10         NaN         NaN   \n",
      "1  A Ashish Reddy 2012-05-04             3   10.000000  100.000000   \n",
      "2  A Ashish Reddy 2012-05-08             8    6.500000  100.000000   \n",
      "3  A Ashish Reddy 2012-05-18            10    7.000000  100.000000   \n",
      "4  A Ashish Reddy 2012-05-20             4    7.750000  137.500000   \n",
      "5  A Ashish Reddy 2013-04-05             7    7.000000  126.000000   \n",
      "6  A Ashish Reddy 2013-04-07            14    7.000000  134.166667   \n",
      "7  A Ashish Reddy 2013-04-09             3    8.000000  131.667143   \n",
      "8  A Ashish Reddy 2013-04-12            16    7.375000  124.583750   \n",
      "9  A Ashish Reddy 2013-04-14             4    8.333333  130.494444   \n",
      "\n",
      "   career_matches  \n",
      "0               0  \n",
      "1               1  \n",
      "2               2  \n",
      "3               3  \n",
      "4               4  \n",
      "5               5  \n",
      "6               6  \n",
      "7               7  \n",
      "8               8  \n",
      "9               9  \n"
     ]
    }
   ],
   "source": [
    "def engineer_career_features(player_match):\n",
    "    \"\"\"Create career average features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3.5 ENGINEERING CAREER FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Career average using expanding mean\n",
    "    career_avg_expanding = player_match.groupby(\"batter\")[\"batsman_runs\"]\\\n",
    "        .expanding().mean().shift(1, fill_value=np.nan)\n",
    "    career_avg_expanding = career_avg_expanding.reset_index(level=0, drop=True)\n",
    "    player_match[\"career_avg\"] = career_avg_expanding\n",
    "    \n",
    "    # Career strike rate using expanding mean\n",
    "    career_sr_expanding = player_match.groupby(\"batter\")[\"strike_rate\"]\\\n",
    "        .expanding().mean().shift(1, fill_value=np.nan)\n",
    "    career_sr_expanding = career_sr_expanding.reset_index(level=0, drop=True)\n",
    "    player_match[\"career_sr\"] = career_sr_expanding\n",
    "    \n",
    "    # Career matches played\n",
    "    player_match[\"career_matches\"] = player_match.groupby(\"batter\").cumcount()\n",
    "    \n",
    "    print(\"Career features created:\")\n",
    "    print(player_match[[\"batter\", \"date\", \"batsman_runs\", \"career_avg\", \"career_sr\", \"career_matches\"]].head(10))\n",
    "    \n",
    "    return player_match\n",
    "\n",
    "# Create career features\n",
    "player_match = engineer_career_features(player_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Target Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:56.785259Z",
     "iopub.status.busy": "2026-02-18T09:39:56.784488Z",
     "iopub.status.idle": "2026-02-18T09:39:56.830328Z",
     "shell.execute_reply": "2026-02-18T09:39:56.827764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. CREATING TARGET LABELS\n",
      "============================================================\n",
      "Data after creating targets: (9054, 24)\n",
      "Target statistics:\n",
      "Next match runs - Mean: 19.72, Std: 20.85\n",
      "Next match SR - Mean: 106.99, Std: 63.58\n"
     ]
    }
   ],
   "source": [
    "def create_target_labels(player_match):\n",
    "    \"\"\"Create target labels (next match performance)\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. CREATING TARGET LABELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create target: runs in next match\n",
    "    player_match = player_match.sort_values([\"batter\", \"date\"])\n",
    "    player_match[\"target_next_runs\"] = player_match.groupby(\"batter\")[\"batsman_runs\"].shift(-1)\n",
    "    \n",
    "    # Create target: strike rate in next match\n",
    "    player_match[\"target_next_sr\"] = player_match.groupby(\"batter\")[\"strike_rate\"].shift(-1)\n",
    "    \n",
    "    # Remove rows where target is NaN (last match for each player)\n",
    "    player_match_clean = player_match.dropna(subset=[\"target_next_runs\"])\n",
    "    \n",
    "    print(f\"Data after creating targets: {player_match_clean.shape}\")\n",
    "    print(f\"Target statistics:\")\n",
    "    print(f\"Next match runs - Mean: {player_match_clean[\"target_next_runs\"].mean():.2f}, Std: {player_match_clean[\"target_next_runs\"].std():.2f}\")\n",
    "    print(f\"Next match SR - Mean: {player_match_clean[\"target_next_sr\"].mean():.2f}, Std: {player_match_clean[\"target_next_sr\"].std():.2f}\")\n",
    "    \n",
    "    return player_match_clean\n",
    "\n",
    "# Create target labels\n",
    "player_match_clean = create_target_labels(player_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:56.836324Z",
     "iopub.status.busy": "2026-02-18T09:39:56.835456Z",
     "iopub.status.idle": "2026-02-18T09:39:56.921500Z",
     "shell.execute_reply": "2026-02-18T09:39:56.920074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. FEATURE SELECTION AND PREPROCESSING\n",
      "============================================================\n",
      "Features shape: (9054, 14)\n",
      "Features selected: ['put_avg', 'put_avg_expanding', 'rolling_avg_5', 'rolling_sr_5', 'venue_avg', 'venue_overall_avg', 'pvp_avg', 'career_avg', 'career_sr', 'career_matches', 'ball', 'is_boundary', 'is_six', 'is_four']\n",
      "Feature statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           put_avg  put_avg_expanding  rolling_avg_5  rolling_sr_5  \\\n",
      "count  9054.000000        9054.000000    9054.000000   9054.000000   \n",
      "mean     19.754829          19.946419      19.972511    107.640728   \n",
      "std      11.717716          14.720824      12.387729     34.774497   \n",
      "min       0.000000           0.000000       0.000000      0.000000   \n",
      "25%      10.666667           8.500000      10.500000     87.228000   \n",
      "50%      20.000000          18.333333      18.500000    107.078000   \n",
      "75%      27.000000          28.000000      27.400000    127.743000   \n",
      "max     120.000000         158.000000     158.000000    400.000000   \n",
      "\n",
      "         venue_avg  venue_overall_avg      pvp_avg   career_avg    career_sr  \\\n",
      "count  9054.000000        9054.000000  9054.000000  9054.000000  9054.000000   \n",
      "mean     19.788203          19.327437     5.683232    19.948605   107.819885   \n",
      "std      13.275306           1.445922     3.294719     9.929360    27.143746   \n",
      "min       0.000000          14.787879     0.000000     0.000000     0.000000   \n",
      "25%       9.600000          19.111732     3.422619    13.606719    96.165874   \n",
      "50%      18.866667          19.530134     5.416667    20.875000   109.904800   \n",
      "75%      28.272727          19.843718     7.500000    26.632895   122.222500   \n",
      "max     114.000000          23.983871    26.500000   158.000000   366.670000   \n",
      "\n",
      "       career_matches         ball  is_boundary       is_six      is_four  \n",
      "count     9054.000000  9054.000000  9054.000000  9054.000000  9054.000000  \n",
      "mean        32.159377    16.121825     2.555666     0.707753     1.843053  \n",
      "std         31.996415    14.000026     3.102745     1.275344     2.323690  \n",
      "min          0.000000     1.000000     0.000000     0.000000     0.000000  \n",
      "25%          7.000000     5.000000     0.000000     0.000000     0.000000  \n",
      "50%         21.000000    12.000000     1.000000     0.000000     1.000000  \n",
      "75%         49.000000    23.000000     4.000000     1.000000     3.000000  \n",
      "max        155.000000    77.000000    30.000000    17.000000    19.000000  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def feature_selection_and_preprocessing(player_match_clean):\n",
    "    \"\"\"Select features and handle preprocessing\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. FEATURE SELECTION AND PREPROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Select features for modeling\n",
    "    feature_columns = [\n",
    "        \"put_avg\", \"put_avg_expanding\",\n",
    "        \"rolling_avg_5\", \"rolling_sr_5\",\n",
    "        \"venue_avg\", \"venue_overall_avg\",\n",
    "        \"pvp_avg\",\n",
    "        \"career_avg\", \"career_sr\", \"career_matches\",\n",
    "        \"ball\", \"is_boundary\", \"is_six\", \"is_four\"\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values in features\n",
    "    for col in feature_columns:\n",
    "        if col in player_match_clean.columns:\n",
    "            player_match_clean[col] = player_match_clean[col].fillna(player_match_clean[col].median())\n",
    "    \n",
    "    features = player_match_clean[feature_columns]\n",
    "    labels_runs = player_match_clean[\"target_next_runs\"]\n",
    "    labels_sr = player_match_clean[\"target_next_sr\"]\n",
    "    \n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Features selected: {feature_columns}\")\n",
    "    print(f\"Feature statistics:\")\n",
    "    print(features.describe())\n",
    "    \n",
    "    return features, labels_runs, labels_sr, feature_columns\n",
    "\n",
    "# Select features and preprocess\n",
    "features, labels_runs, labels_sr, feature_columns = feature_selection_and_preprocessing(player_match_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time-Series Aware Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:56.927451Z",
     "iopub.status.busy": "2026-02-18T09:39:56.926852Z",
     "iopub.status.idle": "2026-02-18T09:39:56.957715Z",
     "shell.execute_reply": "2026-02-18T09:39:56.955616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. TIME-SERIES AWARE TRAIN-TEST SPLIT\n",
      "============================================================\n",
      "Train set size: 7243 (80.0%)\n",
      "Test set size: 1811 (20.0%)\n",
      "Train date range: 2008-04-18 00:00:00 to 2015-05-09 00:00:00\n",
      "Test date range: 2015-05-09 00:00:00 to 2017-05-19 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def time_series_split(features, labels_runs, labels_sr, player_match_clean):\n",
    "    \"\"\"Perform time-series aware train-test split\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. TIME-SERIES AWARE TRAIN-TEST SPLIT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort by date for proper time-series split\n",
    "    player_match_clean = player_match_clean.sort_values(\"date\")\n",
    "    \n",
    "    # Time-series split (80% train, 20% test)\n",
    "    split_idx = int(len(player_match_clean) * 0.8)\n",
    "    \n",
    "    X_train = features[:split_idx]\n",
    "    X_test = features[split_idx:]\n",
    "    y_train_runs = labels_runs[:split_idx]\n",
    "    y_test_runs = labels_runs[split_idx:]\n",
    "    y_train_sr = labels_sr[:split_idx]\n",
    "    y_test_sr = labels_sr[split_idx:]\n",
    "    \n",
    "    print(f\"Train set size: {len(X_train)} ({len(X_train)/len(features)*100:.1f}%)\")\n",
    "    print(f\"Test set size: {len(X_test)} ({len(X_test)/len(features)*100:.1f}%)\")\n",
    "    print(f\"Train date range: {player_match_clean.iloc[:split_idx][\"date\"].min()} to {player_match_clean.iloc[:split_idx][\"date\"].max()}\")\n",
    "    print(f\"Test date range: {player_match_clean.iloc[split_idx:][\"date\"].min()} to {player_match_clean.iloc[split_idx:][\"date\"].max()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train_runs, y_test_runs, y_train_sr, y_test_sr\n",
    "\n",
    "# Perform time-series split\n",
    "X_train, X_test, y_train_runs, y_test_runs, y_train_sr, y_test_sr = time_series_split(\n",
    "    features, labels_runs, labels_sr, player_match_clean\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:56.963650Z",
     "iopub.status.busy": "2026-02-18T09:39:56.963107Z",
     "iopub.status.idle": "2026-02-18T09:39:57.000358Z",
     "shell.execute_reply": "2026-02-18T09:39:56.998020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. CREATING FEATURE PIPELINE\n",
      "============================================================\n",
      "Feature pipeline created and applied:\n",
      "Scaled training data shape: (7243, 14)\n",
      "Scaled test data shape: (1811, 14)\n",
      "Sample scaled features:\n",
      "    put_avg  put_avg_expanding  rolling_avg_5  rolling_sr_5  venue_avg  \\\n",
      "0 -0.502035          -0.749183      -0.780673     -0.207564  -0.709098   \n",
      "1 -0.374855          -0.084118      -1.063708     -0.207564   0.003366   \n",
      "2 -0.600954          -0.444831      -1.023275     -0.207564  -0.825001   \n",
      "3 -0.600954          -0.489921      -0.962624      0.861467  -0.825001   \n",
      "4 -0.714003          -0.489921      -1.023275      0.533631  -0.825001   \n",
      "\n",
      "   venue_overall_avg   pvp_avg  career_avg  career_sr  career_matches  \\\n",
      "0          -0.004717 -0.523122    0.118640   0.115093       -1.000485   \n",
      "1           0.534424 -0.716012   -0.956843  -0.253947       -0.968057   \n",
      "2           0.196329 -1.093406   -1.302976  -0.253947       -0.935629   \n",
      "3           0.196329 -0.489576   -1.253529  -0.253947       -0.903202   \n",
      "4           0.196329  3.737235   -1.179357   1.143253       -0.870774   \n",
      "\n",
      "       ball  is_boundary    is_six   is_four  \n",
      "0 -0.424532    -0.489593  0.245268 -0.786288  \n",
      "1 -0.924257    -0.815703 -0.545332 -0.786288  \n",
      "2 -0.567310    -0.489593 -0.545332 -0.350951  \n",
      "3 -0.852868    -0.163483 -0.545332  0.084387  \n",
      "4 -0.781479    -0.815703 -0.545332 -0.786288  \n"
     ]
    }
   ],
   "source": [
    "def create_feature_pipeline(X_train, X_test, feature_columns):\n",
    "    \"\"\"Create and apply feature preprocessing pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. CREATING FEATURE PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    feature_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Fit pipeline on training data\n",
    "    X_train_scaled = feature_pipeline.fit_transform(X_train)\n",
    "    X_test_scaled = feature_pipeline.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame for easier handling\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns)\n",
    "    \n",
    "    print(\"Feature pipeline created and applied:\")\n",
    "    print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Scaled test data shape: {X_test_scaled.shape}\")\n",
    "    print(f\"Sample scaled features:\")\n",
    "    print(X_train_scaled.head())\n",
    "    \n",
    "    return feature_pipeline, X_train_scaled, X_test_scaled\n",
    "\n",
    "# Create feature pipeline\n",
    "feature_pipeline, X_train_scaled, X_test_scaled = create_feature_pipeline(X_train, X_test, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:57.006833Z",
     "iopub.status.busy": "2026-02-18T09:39:57.006252Z",
     "iopub.status.idle": "2026-02-18T09:39:57.649823Z",
     "shell.execute_reply": "2026-02-18T09:39:57.647499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "8. SAVING ARTIFACTS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved successfully:\n",
      "- feature_pipeline.pkl: Preprocessing pipeline\n",
      "- dataset.csv: Complete feature-engineered dataset\n",
      "- train_data.csv: Training set\n",
      "- test_data.csv: Test set\n"
     ]
    }
   ],
   "source": [
    "def save_artifacts(feature_pipeline, final_dataset, X_train, X_test, y_train_runs, y_test_runs, y_train_sr, y_test_sr, feature_columns):\n",
    "    \"\"\"Save all artifacts\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. SAVING ARTIFACTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save the feature pipeline\n",
    "    joblib.dump(feature_pipeline, \"feature_pipeline.pkl\")\n",
    "    \n",
    "    # Save the final dataset with all features and targets\n",
    "    final_dataset.to_csv(\"data/dataset.csv\", index=False)\n",
    "    \n",
    "    # Save train-test splits\n",
    "    train_data = pd.concat([X_train, y_train_runs, y_train_sr], axis=1)\n",
    "    train_data.columns = feature_columns + [\"target_runs\", \"target_sr\"]\n",
    "    train_data.to_csv(\"data/train_data.csv\", index=False)\n",
    "    \n",
    "    test_data = pd.concat([X_test, y_test_runs, y_test_sr], axis=1)\n",
    "    test_data.columns = feature_columns + [\"target_runs\", \"target_sr\"]\n",
    "    test_data.to_csv(\"data/test_data.csv\", index=False)\n",
    "    \n",
    "    print(\"Artifacts saved successfully:\")\n",
    "    print(\"- feature_pipeline.pkl: Preprocessing pipeline\")\n",
    "    print(\"- dataset.csv: Complete feature-engineered dataset\")\n",
    "    print(\"- train_data.csv: Training set\")\n",
    "    print(\"- test_data.csv: Test set\")\n",
    "\n",
    "# Save all artifacts\n",
    "save_artifacts(\n",
    "    feature_pipeline, player_match_clean, X_train, X_test, \n",
    "    y_train_runs, y_test_runs, y_train_sr, y_test_sr, feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. WICKET FEATURE ENGINEERING\n",
    "\n",
    "This section adds comprehensive wicket prediction features to complement the batting performance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:39:57.656492Z",
     "iopub.status.busy": "2026-02-18T09:39:57.655609Z",
     "iopub.status.idle": "2026-02-18T09:40:00.394791Z",
     "shell.execute_reply": "2026-02-18T09:40:00.393078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "9. ENGINEERING WICKET FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wicket features created:\n",
      "Shape: (9054, 37)\n",
      "Sample wicket features:\n",
      "           batter  wickets_lost  put_wicket_rate  venue_wicket_rate  \\\n",
      "0  A Ashish Reddy           1.0         1.000000                1.0   \n",
      "1  A Ashish Reddy           1.0         0.666667                0.5   \n",
      "2  A Ashish Reddy           1.0         0.333333                0.7   \n",
      "3  A Ashish Reddy           0.0         0.333333                0.7   \n",
      "4  A Ashish Reddy           1.0         1.000000                0.7   \n",
      "5  A Ashish Reddy           0.0         0.000000                0.7   \n",
      "6  A Ashish Reddy           1.0         1.000000                0.7   \n",
      "7  A Ashish Reddy           1.0         1.000000                1.0   \n",
      "8  A Ashish Reddy           1.0         1.000000                1.0   \n",
      "9  A Ashish Reddy           1.0         1.000000                1.0   \n",
      "\n",
      "   rolling_wicket_rate_5  career_wicket_rate  \n",
      "0                   1.00            0.000000  \n",
      "1                   1.00            1.000000  \n",
      "2                   1.00            1.000000  \n",
      "3                   0.75            1.000000  \n",
      "4                   0.80            0.750000  \n",
      "5                   0.60            0.800000  \n",
      "6                   0.60            0.666667  \n",
      "7                   0.60            0.714286  \n",
      "8                   0.80            0.750000  \n",
      "9                   0.80            0.777778  \n"
     ]
    }
   ],
   "source": [
    "def engineer_wicket_features(player_match, df):\n",
    "    \"\"\"Create comprehensive wicket prediction features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. ENGINEERING WICKET FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, create wicket-specific data at player-match level\n",
    "    # Get wicket data for each player in each match\n",
    "    wicket_data = df[df['player_dismissed'].notna()].copy()\n",
    "    \n",
    "    # Create dismissal type mapping for common categories\n",
    "    dismissal_mapping = {\n",
    "        'caught': 'caught',\n",
    "        'bowled': 'bowled', \n",
    "        'run out': 'run_out',\n",
    "        'lbw': 'lbw',\n",
    "        'stumped': 'stumped',\n",
    "        'caught and bowled': 'caught_and_bowled',\n",
    "        'retired hurt': 'retired_hurt',\n",
    "        'hit wicket': 'hit_wicket',\n",
    "        'obstructing the field': 'obstructing'\n",
    "    }\n",
    "    \n",
    "    # Map dismissal types\n",
    "    wicket_data['dismissal_type_mapped'] = wicket_data['dismissal_kind'].map(dismissal_mapping).fillna('other')\n",
    "    \n",
    "    # Create wicket features for each player-match\n",
    "    player_wickets = wicket_data.groupby(['player_dismissed', 'match_id', 'date', 'venue', 'bowling_team']).agg({\n",
    "        'dismissal_kind': 'count',  # Total wickets\n",
    "        'dismissal_type_mapped': lambda x: (x == 'caught').sum(),  # Caught dismissals\n",
    "        'over': 'mean',  # Average over when dismissed\n",
    "        'ball': 'mean'   # Average ball when dismissed\n",
    "    }).reset_index()\n",
    "    \n",
    "    player_wickets.columns = ['batter', 'match_id', 'date', 'venue', 'bowling_team', \n",
    "                             'wickets_lost', 'caught_dismissals', 'avg_over_dismissed', 'avg_ball_dismissed']\n",
    "    \n",
    "    # Merge wicket data back to player_match\n",
    "    player_match_wickets = player_match.merge(\n",
    "        player_wickets, \n",
    "        on=['batter', 'match_id', 'date', 'venue', 'bowling_team'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing wicket data with 0 (no wickets in that match)\n",
    "    player_match_wickets['wickets_lost'] = player_match_wickets['wickets_lost'].fillna(0)\n",
    "    player_match_wickets['caught_dismissals'] = player_match_wickets['caught_dismissals'].fillna(0)\n",
    "    player_match_wickets['avg_over_dismissed'] = player_match_wickets['avg_over_dismissed'].fillna(20)  # Default to last over\n",
    "    player_match_wickets['avg_ball_dismissed'] = player_match_wickets['avg_ball_dismissed'].fillna(6)   # Default to last ball\n",
    "    \n",
    "    # Create wicket probability features\n",
    "    # PUT wicket rate (Player vs Team wicket rate)\n",
    "    player_match_wickets['put_wicket_rate'] = player_match_wickets.groupby(['batter', 'bowling_team'])['wickets_lost'].transform('mean')\n",
    "    \n",
    "    # PUT wicket rate expanding (cumulative)\n",
    "    put_wicket_expanding = player_match_wickets.groupby(['batter', 'bowling_team'])['wickets_lost']\\\n",
    "        .expanding().mean().shift(1, fill_value=0)\n",
    "    put_wicket_expanding = put_wicket_expanding.reset_index(level=[0,1], drop=True)\n",
    "    player_match_wickets['put_wicket_rate_expanding'] = put_wicket_expanding\n",
    "    \n",
    "    # Venue wicket rate\n",
    "    player_match_wickets['venue_wicket_rate'] = player_match_wickets.groupby(['batter', 'venue'])['wickets_lost'].transform('mean')\n",
    "    \n",
    "    # Overall venue wicket rate\n",
    "    player_match_wickets['venue_overall_wicket_rate'] = player_match_wickets.groupby('venue')['wickets_lost'].transform('mean')\n",
    "    \n",
    "    # Rolling wicket features (last 5 matches)\n",
    "    player_match_wickets['rolling_wicket_rate_5'] = player_match_wickets.groupby('batter')['wickets_lost']\\\n",
    "        .rolling(5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Career wicket rate (expanding)\n",
    "    career_wicket_expanding = player_match_wickets.groupby('batter')['wickets_lost']\\\n",
    "        .expanding().mean().shift(1, fill_value=0)\n",
    "    career_wicket_expanding = career_wicket_expanding.reset_index(level=0, drop=True)\n",
    "    player_match_wickets['career_wicket_rate'] = career_wicket_expanding\n",
    "    \n",
    "    # Dismissal type features\n",
    "    player_match_wickets['caught_rate'] = player_match_wickets.groupby('batter')['caught_dismissals']\\\n",
    "        .expanding().mean().shift(1, fill_value=0).reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Bowling strength features (quality of opposition bowling)\n",
    "    bowling_strength = df.groupby(['bowling_team', 'match_id']).agg({\n",
    "        'is_wicket': 'sum',\n",
    "        'total_runs': 'mean'\n",
    "    }).reset_index()\n",
    "    bowling_strength.columns = ['bowling_team', 'match_id', 'team_wickets', 'avg_runs_conceded']\n",
    "    \n",
    "    # Merge bowling strength\n",
    "    player_match_wickets = player_match_wickets.merge(\n",
    "        bowling_strength, \n",
    "        on=['bowling_team', 'match_id'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(\"Wicket features created:\")\n",
    "    print(f\"Shape: {player_match_wickets.shape}\")\n",
    "    print(f\"Sample wicket features:\")\n",
    "    print(player_match_wickets[['batter', 'wickets_lost', 'put_wicket_rate', 'venue_wicket_rate', \n",
    "                               'rolling_wicket_rate_5', 'career_wicket_rate']].head(10))\n",
    "    \n",
    "    return player_match_wickets\n",
    "\n",
    "# Create wicket features\n",
    "player_match_wickets = engineer_wicket_features(player_match_clean, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:40:00.400334Z",
     "iopub.status.busy": "2026-02-18T09:40:00.399316Z",
     "iopub.status.idle": "2026-02-18T09:40:00.434070Z",
     "shell.execute_reply": "2026-02-18T09:40:00.432404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "10. CREATING WICKET TARGET LABELS\n",
      "============================================================\n",
      "Data after creating wicket targets: (9054, 40)\n",
      "Wicket target statistics:\n",
      "Next match wicket rate: 0.745\n",
      "Next match caught rate: 0.445\n",
      "Wicket distribution:\n",
      "target_next_wicket\n",
      "1    0.744864\n",
      "0    0.255136\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## 10. WICKET TARGET LABELS\n",
    "\n",
    "def create_wicket_targets(player_match_wickets):\n",
    "    \"\"\"Create wicket prediction target labels\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"10. CREATING WICKET TARGET LABELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort by batter and date for target creation\n",
    "    player_match_wickets = player_match_wickets.sort_values(['batter', 'date'])\n",
    "    \n",
    "    # Create target: wickets in next match (binary - will player get out?)\n",
    "    player_match_wickets['target_next_wicket'] = (player_match_wickets.groupby('batter')['wickets_lost'].shift(-1) > 0).astype(int)\n",
    "    \n",
    "    # Create target: probability of getting out (continuous)\n",
    "    player_match_wickets['target_next_wicket_prob'] = player_match_wickets.groupby('batter')['wickets_lost'].shift(-1)\n",
    "    \n",
    "    # Create target: caught dismissal probability\n",
    "    player_match_wickets['target_next_caught'] = (player_match_wickets.groupby('batter')['caught_dismissals'].shift(-1) > 0).astype(int)\n",
    "    \n",
    "    # Remove rows where target is NaN (last match for each player)\n",
    "    player_match_wickets_clean = player_match_wickets.dropna(subset=['target_next_wicket'])\n",
    "    \n",
    "    print(f\"Data after creating wicket targets: {player_match_wickets_clean.shape}\")\n",
    "    print(f\"Wicket target statistics:\")\n",
    "    print(f\"Next match wicket rate: {player_match_wickets_clean['target_next_wicket'].mean():.3f}\")\n",
    "    print(f\"Next match caught rate: {player_match_wickets_clean['target_next_caught'].mean():.3f}\")\n",
    "    print(f\"Wicket distribution:\")\n",
    "    print(player_match_wickets_clean['target_next_wicket'].value_counts(normalize=True))\n",
    "    \n",
    "    return player_match_wickets_clean\n",
    "\n",
    "# Create wicket target labels\n",
    "player_match_wickets_clean = create_wicket_targets(player_match_wickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:40:00.439887Z",
     "iopub.status.busy": "2026-02-18T09:40:00.439333Z",
     "iopub.status.idle": "2026-02-18T09:40:00.580461Z",
     "shell.execute_reply": "2026-02-18T09:40:00.578865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "11. COMBINED FEATURE SELECTION AND PREPROCESSING\n",
      "============================================================\n",
      "Combined features shape: (9054, 25)\n",
      "Batting features: 14\n",
      "Wicket features: 11\n",
      "Total features: 25\n",
      "\n",
      "Feature statistics:\n",
      "           put_avg  put_avg_expanding  rolling_avg_5  rolling_sr_5  \\\n",
      "count  9054.000000        9054.000000    9054.000000   9054.000000   \n",
      "mean     19.754829          19.946419      19.972511    107.640728   \n",
      "std      11.717716          14.720824      12.387729     34.774497   \n",
      "min       0.000000           0.000000       0.000000      0.000000   \n",
      "25%      10.666667           8.500000      10.500000     87.228000   \n",
      "50%      20.000000          18.333333      18.500000    107.078000   \n",
      "75%      27.000000          28.000000      27.400000    127.743000   \n",
      "max     120.000000         158.000000     158.000000    400.000000   \n",
      "\n",
      "         venue_avg  venue_overall_avg      pvp_avg   career_avg    career_sr  \\\n",
      "count  9054.000000        9054.000000  9054.000000  9054.000000  9054.000000   \n",
      "mean     19.788203          19.327437     5.683232    19.948605   107.819885   \n",
      "std      13.275306           1.445922     3.294719     9.929360    27.143746   \n",
      "min       0.000000          14.787879     0.000000     0.000000     0.000000   \n",
      "25%       9.600000          19.111732     3.422619    13.606719    96.165874   \n",
      "50%      18.866667          19.530134     5.416667    20.875000   109.904800   \n",
      "75%      28.272727          19.843718     7.500000    26.632895   122.222500   \n",
      "max     114.000000          23.983871    26.500000   158.000000   366.670000   \n",
      "\n",
      "       career_matches  ...  put_wicket_rate_expanding  venue_wicket_rate  \\\n",
      "count     9054.000000  ...                9054.000000        9054.000000   \n",
      "mean        32.159377  ...                   0.777164           0.779435   \n",
      "std         31.996415  ...                   0.307282           0.287458   \n",
      "min          0.000000  ...                   0.000000           0.000000   \n",
      "25%          7.000000  ...                   0.666667           0.666667   \n",
      "50%         21.000000  ...                   0.928571           0.879310   \n",
      "75%         49.000000  ...                   1.000000           1.000000   \n",
      "max        155.000000  ...                   2.000000           2.000000   \n",
      "\n",
      "       venue_overall_wicket_rate  rolling_wicket_rate_5  career_wicket_rate  \\\n",
      "count                9054.000000            9054.000000         9054.000000   \n",
      "mean                    0.779435               0.778676            0.777017   \n",
      "std                     0.018824               0.246033            0.198311   \n",
      "min                     0.701754               0.000000            0.000000   \n",
      "25%                     0.767606               0.600000            0.681818   \n",
      "50%                     0.778751               0.800000            0.833333   \n",
      "75%                     0.786932               1.000000            0.909091   \n",
      "max                     0.862745               2.000000            2.000000   \n",
      "\n",
      "       caught_rate  avg_over_dismissed  avg_ball_dismissed  team_wickets  \\\n",
      "count  9054.000000         9054.000000         9054.000000   9054.000000   \n",
      "mean      0.453185           13.700574            4.169980      6.534902   \n",
      "std       0.205412            6.274936            1.855857      2.345890   \n",
      "min       0.000000            1.000000            1.000000      0.000000   \n",
      "25%       0.357143            8.000000            3.000000      5.000000   \n",
      "50%       0.472222           16.000000            4.000000      7.000000   \n",
      "75%       0.571429           20.000000            6.000000      8.000000   \n",
      "max       1.000000           20.000000            9.000000     12.000000   \n",
      "\n",
      "       avg_runs_conceded  \n",
      "count        9054.000000  \n",
      "mean            1.263301  \n",
      "std             0.227792  \n",
      "min             0.285714  \n",
      "25%             1.106796  \n",
      "50%             1.264000  \n",
      "75%             1.404959  \n",
      "max             2.305556  \n",
      "\n",
      "[8 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "## 11. COMBINED FEATURE SELECTION AND PREPROCESSING\n",
    "\n",
    "def combined_feature_selection_and_preprocessing(player_match_wickets_clean):\n",
    "    \"\"\"Select and preprocess all features for both batting and wicket prediction\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"11. COMBINED FEATURE SELECTION AND PREPROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Batting features (original)\n",
    "    batting_features = [\n",
    "        \"put_avg\", \"put_avg_expanding\",\n",
    "        \"rolling_avg_5\", \"rolling_sr_5\",\n",
    "        \"venue_avg\", \"venue_overall_avg\",\n",
    "        \"pvp_avg\",\n",
    "        \"career_avg\", \"career_sr\", \"career_matches\",\n",
    "        \"ball\", \"is_boundary\", \"is_six\", \"is_four\"\n",
    "    ]\n",
    "    \n",
    "    # New wicket features\n",
    "    wicket_features = [\n",
    "        \"put_wicket_rate\", \"put_wicket_rate_expanding\",\n",
    "        \"venue_wicket_rate\", \"venue_overall_wicket_rate\", \n",
    "        \"rolling_wicket_rate_5\", \"career_wicket_rate\",\n",
    "        \"caught_rate\", \"avg_over_dismissed\", \"avg_ball_dismissed\",\n",
    "        \"team_wickets\", \"avg_runs_conceded\"\n",
    "    ]\n",
    "    \n",
    "    # Combined feature list\n",
    "    all_features = batting_features + wicket_features\n",
    "    \n",
    "    # Handle missing values in features\n",
    "    for col in all_features:\n",
    "        if col in player_match_wickets_clean.columns:\n",
    "            player_match_wickets_clean[col] = player_match_wickets_clean[col].fillna(player_match_wickets_clean[col].median())\n",
    "    \n",
    "    # Create feature matrices\n",
    "    features = player_match_wickets_clean[all_features]\n",
    "    \n",
    "    # Batting targets\n",
    "    labels_runs = player_match_wickets_clean[\"target_next_runs\"]\n",
    "    labels_sr = player_match_wickets_clean[\"target_next_sr\"]\n",
    "    \n",
    "    # Wicket targets\n",
    "    labels_wicket = player_match_wickets_clean[\"target_next_wicket\"]\n",
    "    labels_wicket_prob = player_match_wickets_clean[\"target_next_wicket_prob\"]\n",
    "    labels_caught = player_match_wickets_clean[\"target_next_caught\"]\n",
    "    \n",
    "    print(f\"Combined features shape: {features.shape}\")\n",
    "    print(f\"Batting features: {len(batting_features)}\")\n",
    "    print(f\"Wicket features: {len(wicket_features)}\")\n",
    "    print(f\"Total features: {len(all_features)}\")\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(features.describe())\n",
    "    \n",
    "    return (features, labels_runs, labels_sr, labels_wicket, labels_wicket_prob, labels_caught, \n",
    "            all_features, batting_features, wicket_features)\n",
    "\n",
    "# Select and preprocess all features\n",
    "(features, labels_runs, labels_sr, labels_wicket, labels_wicket_prob, labels_caught, \n",
    " all_features, batting_features, wicket_features) = combined_feature_selection_and_preprocessing(player_match_wickets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:40:00.586976Z",
     "iopub.status.busy": "2026-02-18T09:40:00.586333Z",
     "iopub.status.idle": "2026-02-18T09:40:00.619455Z",
     "shell.execute_reply": "2026-02-18T09:40:00.617723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "12. COMBINED TIME-SERIES TRAIN-TEST SPLIT\n",
      "============================================================\n",
      "Train set size: 7243 (80.0%)\n",
      "Test set size: 1811 (20.0%)\n",
      "Train date range: 2008-04-18 00:00:00 to 2015-05-09 00:00:00\n",
      "Test date range: 2015-05-09 00:00:00 to 2017-05-19 00:00:00\n",
      "\n",
      "Target distributions in train set:\n",
      "Runs - Mean: 19.37, Std: 20.64\n",
      "Strike Rate - Mean: 106.80, Std: 64.22\n",
      "Wicket Rate: 0.740\n",
      "Caught Rate: 0.436\n"
     ]
    }
   ],
   "source": [
    "## 12. COMBINED TIME-SERIES TRAIN-TEST SPLIT\n",
    "\n",
    "def combined_time_series_split(features, labels_runs, labels_sr, labels_wicket, \n",
    "                               labels_wicket_prob, labels_caught, player_match_wickets_clean):\n",
    "    \"\"\"Perform time-series aware train-test split for all targets\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"12. COMBINED TIME-SERIES TRAIN-TEST SPLIT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort by date for proper time-series split\n",
    "    player_match_wickets_clean = player_match_wickets_clean.sort_values(\"date\")\n",
    "    \n",
    "    # Time-series split (80% train, 20% test)\n",
    "    split_idx = int(len(player_match_wickets_clean) * 0.8)\n",
    "    \n",
    "    # Split features\n",
    "    X_train = features[:split_idx]\n",
    "    X_test = features[split_idx:]\n",
    "    \n",
    "    # Split batting targets\n",
    "    y_train_runs = labels_runs[:split_idx]\n",
    "    y_test_runs = labels_runs[split_idx:]\n",
    "    y_train_sr = labels_sr[:split_idx]\n",
    "    y_test_sr = labels_sr[split_idx:]\n",
    "    \n",
    "    # Split wicket targets\n",
    "    y_train_wicket = labels_wicket[:split_idx]\n",
    "    y_test_wicket = labels_wicket[split_idx:]\n",
    "    y_train_wicket_prob = labels_wicket_prob[:split_idx]\n",
    "    y_test_wicket_prob = labels_wicket_prob[split_idx:]\n",
    "    y_train_caught = labels_caught[:split_idx]\n",
    "    y_test_caught = labels_caught[split_idx:]\n",
    "    \n",
    "    print(f\"Train set size: {len(X_train)} ({len(X_train)/len(features)*100:.1f}%)\")\n",
    "    print(f\"Test set size: {len(X_test)} ({len(X_test)/len(features)*100:.1f}%)\")\n",
    "    print(f\"Train date range: {player_match_wickets_clean.iloc[:split_idx]['date'].min()} to {player_match_wickets_clean.iloc[:split_idx]['date'].max()}\")\n",
    "    print(f\"Test date range: {player_match_wickets_clean.iloc[split_idx:]['date'].min()} to {player_match_wickets_clean.iloc[split_idx:]['date'].max()}\")\n",
    "    \n",
    "    print(f\"\\nTarget distributions in train set:\")\n",
    "    print(f\"Runs - Mean: {y_train_runs.mean():.2f}, Std: {y_train_runs.std():.2f}\")\n",
    "    print(f\"Strike Rate - Mean: {y_train_sr.mean():.2f}, Std: {y_train_sr.std():.2f}\")\n",
    "    print(f\"Wicket Rate: {y_train_wicket.mean():.3f}\")\n",
    "    print(f\"Caught Rate: {y_train_caught.mean():.3f}\")\n",
    "    \n",
    "    return (X_train, X_test, y_train_runs, y_test_runs, y_train_sr, y_test_sr,\n",
    "            y_train_wicket, y_test_wicket, y_train_wicket_prob, y_test_wicket_prob,\n",
    "            y_train_caught, y_test_caught)\n",
    "\n",
    "# Perform combined time-series split\n",
    "(X_train, X_test, y_train_runs, y_test_runs, y_train_sr, y_test_sr,\n",
    " y_train_wicket, y_test_wicket, y_train_wicket_prob, y_test_wicket_prob,\n",
    " y_train_caught, y_test_caught) = combined_time_series_split(\n",
    "    features, labels_runs, labels_sr, labels_wicket, labels_wicket_prob, labels_caught, player_match_wickets_clean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:40:00.625295Z",
     "iopub.status.busy": "2026-02-18T09:40:00.624638Z",
     "iopub.status.idle": "2026-02-18T09:40:00.657496Z",
     "shell.execute_reply": "2026-02-18T09:40:00.655155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "13. CREATING COMBINED FEATURE PIPELINE\n",
      "============================================================\n",
      "Combined feature pipeline created and applied:\n",
      "Scaled training data shape: (7243, 25)\n",
      "Scaled test data shape: (1811, 25)\n",
      "Sample scaled features (first 5):\n",
      "    put_avg  put_avg_expanding  rolling_avg_5  rolling_sr_5  venue_avg\n",
      "0 -0.502035          -0.749183      -0.780673     -0.207564  -0.709098\n",
      "1 -0.374855          -0.084118      -1.063708     -0.207564   0.003366\n",
      "2 -0.600954          -0.444831      -1.023275     -0.207564  -0.825001\n",
      "3 -0.600954          -0.489921      -0.962624      0.861467  -0.825001\n",
      "4 -0.714003          -0.489921      -1.023275      0.533631  -0.825001\n"
     ]
    }
   ],
   "source": [
    "## 13. COMBINED FEATURE PIPELINE\n",
    "\n",
    "def create_combined_feature_pipeline(X_train, X_test, all_features):\n",
    "    \"\"\"Create and apply feature preprocessing pipeline for all features\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"13. CREATING COMBINED FEATURE PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    combined_feature_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Fit pipeline on training data\n",
    "    X_train_scaled = combined_feature_pipeline.fit_transform(X_train)\n",
    "    X_test_scaled = combined_feature_pipeline.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame for easier handling\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=all_features)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=all_features)\n",
    "    \n",
    "    print(\"Combined feature pipeline created and applied:\")\n",
    "    print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Scaled test data shape: {X_test_scaled.shape}\")\n",
    "    print(f\"Sample scaled features (first 5):\")\n",
    "    print(X_train_scaled.iloc[:, :5].head())\n",
    "    \n",
    "    return combined_feature_pipeline, X_train_scaled, X_test_scaled\n",
    "\n",
    "# Create combined feature pipeline\n",
    "combined_feature_pipeline, X_train_scaled, X_test_scaled = create_combined_feature_pipeline(X_train, X_test, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T09:40:00.663758Z",
     "iopub.status.busy": "2026-02-18T09:40:00.663141Z",
     "iopub.status.idle": "2026-02-18T09:40:02.088092Z",
     "shell.execute_reply": "2026-02-18T09:40:02.086226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "14. SAVING COMBINED ARTIFACTS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined artifacts saved successfully:\n",
      "- combined_feature_pipeline.pkl: Preprocessing pipeline for all features\n",
      "- combined_dataset.csv: Complete dataset with batting and wicket features\n",
      "- combined_train_data.csv: Training set with all targets\n",
      "- combined_test_data.csv: Test set with all targets\n",
      "- feature_info.json: Feature configuration and metadata\n",
      "\n",
      "Final Summary:\n",
      "- Total features: 25 (14 batting + 11 wicket)\n",
      "- Training samples: 7243\n",
      "- Test samples: 1811\n",
      "- Targets available: runs, strike rate, wicket (binary), wicket probability, caught dismissal\n"
     ]
    }
   ],
   "source": [
    "## 14. SAVING COMBINED ARTIFACTS\n",
    "\n",
    "def save_combined_artifacts(combined_feature_pipeline, final_dataset, X_train_scaled, X_test_scaled,\n",
    "                           y_train_runs, y_test_runs, y_train_sr, y_test_sr,\n",
    "                           y_train_wicket, y_test_wicket, y_train_wicket_prob, y_test_wicket_prob,\n",
    "                           y_train_caught, y_test_caught, all_features, batting_features, wicket_features):\n",
    "    \"\"\"Save all combined artifacts for both batting and wicket prediction\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"14. SAVING COMBINED ARTIFACTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save the combined feature pipeline\n",
    "    joblib.dump(combined_feature_pipeline, \"combined_feature_pipeline.pkl\")\n",
    "    \n",
    "    # Save the final dataset with all features and targets\n",
    "    final_dataset.to_csv(\"data/combined_dataset.csv\", index=False)\n",
    "    \n",
    "    # Save combined training data with all targets\n",
    "    train_data_combined = pd.concat([\n",
    "        X_train_scaled,\n",
    "        y_train_runs.rename('target_runs'),\n",
    "        y_train_sr.rename('target_sr'),\n",
    "        y_train_wicket.rename('target_wicket'),\n",
    "        y_train_wicket_prob.rename('target_wicket_prob'),\n",
    "        y_train_caught.rename('target_caught')\n",
    "    ], axis=1)\n",
    "    train_data_combined.to_csv(\"data/combined_train_data.csv\", index=False)\n",
    "    \n",
    "    # Save combined test data with all targets\n",
    "    test_data_combined = pd.concat([\n",
    "        X_test_scaled,\n",
    "        y_test_runs.rename('target_runs'),\n",
    "        y_test_sr.rename('target_sr'),\n",
    "        y_test_wicket.rename('target_wicket'),\n",
    "        y_test_wicket_prob.rename('target_wicket_prob'),\n",
    "        y_test_caught.rename('target_caught')\n",
    "    ], axis=1)\n",
    "    test_data_combined.to_csv(\"data/combined_test_data.csv\", index=False)\n",
    "    \n",
    "    # Save feature lists for reference\n",
    "    feature_info = {\n",
    "        'all_features': all_features,\n",
    "        'batting_features': batting_features,\n",
    "        'wicket_features': wicket_features,\n",
    "        'total_features': len(all_features)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(\"feature_info.json\", \"w\") as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "    \n",
    "    print(\"Combined artifacts saved successfully:\")\n",
    "    print(\"- combined_feature_pipeline.pkl: Preprocessing pipeline for all features\")\n",
    "    print(\"- combined_dataset.csv: Complete dataset with batting and wicket features\")\n",
    "    print(\"- combined_train_data.csv: Training set with all targets\")\n",
    "    print(\"- combined_test_data.csv: Test set with all targets\")\n",
    "    print(\"- feature_info.json: Feature configuration and metadata\")\n",
    "    \n",
    "    print(f\"\\nFinal Summary:\")\n",
    "    print(f\"- Total features: {len(all_features)} ({len(batting_features)} batting + {len(wicket_features)} wicket)\")\n",
    "    print(f\"- Training samples: {len(X_train_scaled)}\")\n",
    "    print(f\"- Test samples: {len(X_test_scaled)}\")\n",
    "    print(f\"- Targets available: runs, strike rate, wicket (binary), wicket probability, caught dismissal\")\n",
    "\n",
    "# Save all combined artifacts\n",
    "save_combined_artifacts(\n",
    "    combined_feature_pipeline, player_match_wickets_clean, X_train_scaled, X_test_scaled,\n",
    "    y_train_runs, y_test_runs, y_train_sr, y_test_sr,\n",
    "    y_train_wicket, y_test_wicket, y_train_wicket_prob, y_test_wicket_prob,\n",
    "    y_train_caught, y_test_caught, all_features, batting_features, wicket_features\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
